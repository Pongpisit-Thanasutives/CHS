{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28567c5-c0e2-4efa-98a0-63961e04e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Author: Pongpisit Thanasutives ###\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from tqdm import trange\n",
    "from scipy import io as sio\n",
    "from scipy.stats import uniform, norm\n",
    "import pysindy as ps\n",
    "import pocomc as pc\n",
    "from bayesian_model_evidence import log_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ae7ab1-29da-46ac-b917-f1c6ead4b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### algorithm 2 ###\n",
    "def TopRsq(X_full, y, m, n_tops=25):\n",
    "    n_feats = X_full.shape[-1]\n",
    "    r_scores = []\n",
    "    models = []\n",
    "    for comb in combinations(range(n_feats), m):\n",
    "        comb = list(comb)\n",
    "        active_indices = np.zeros(n_feats)\n",
    "        active_indices[comb] = 1\n",
    "        X_sub = X_full[:, comb]\n",
    "        lr = LinearRegression(fit_intercept=False).fit(X_sub, y)\n",
    "        R2 = lr.score(X_sub, y)\n",
    "        r_scores.append(R2)\n",
    "        models.append(active_indices)\n",
    "    r_scores = np.array(r_scores)\n",
    "    r_argsort = np.argsort(r_scores)[::-1][:n_tops]\n",
    "    r_scores = r_scores[r_argsort]\n",
    "    models = np.array(models)\n",
    "    models = models[r_argsort].T\n",
    "    rating = np.dot(models, r_scores)\n",
    "    return models, r_scores, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67516471-5f91-4578-a979-61fc4a2c82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### algorithm 3 ###\n",
    "def comprehensive_search(X_full, y, max_support_size=8, n_tops=None, threshold=0.75):\n",
    "    X = X_full.copy()\n",
    "    n_feats = X_full.shape[-1]\n",
    "    n_tops = int(np.ceil(n_feats/2)) if n_tops is None else n_tops\n",
    "    ratings = np.zeros((n_feats, max_support_size))\n",
    "    search = True; support_size = 1\n",
    "    optimal_indices = None\n",
    "    i0 = np.arange(n_feats)\n",
    "    while search and support_size <= max_support_size:\n",
    "        _, _, rating = TopRsq(X, y, m=support_size, n_tops=n_tops)\n",
    "        rating = rating/rating.max()\n",
    "        ratings[:, support_size-1][i0] = rating\n",
    "        if support_size >= 2:\n",
    "            i0 = np.nonzero(ratings[:, support_size-1] + ratings[:, support_size-2])[0]\n",
    "            X = X_full[:, i0]\n",
    "            i1 = np.where(ratings[:, support_size-1] > threshold)[0]\n",
    "            i2 = np.where(ratings[:, support_size-2] > threshold)[0]\n",
    "            if len(i1) == len(i2) and np.all(i1 == i2):\n",
    "                search = False\n",
    "                optimal_indices = i1\n",
    "        support_size += 1\n",
    "    return optimal_indices, ratings[:, :support_size-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d70184eb-b539-4ae7-9fd5-4eb818c6ddf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.89"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_experiments = 100\n",
    "n_samples = 10000\n",
    "n_features = 8\n",
    "n_informative = 2\n",
    "\n",
    "threshold = 0.25\n",
    "max_support_size = 8\n",
    "\n",
    "success = 0\n",
    "for i in trange(n_experiments):\n",
    "    X_train, y_train = make_regression(n_samples=n_samples, n_features=n_features, n_informative=n_informative)\n",
    "    X_train = StandardScaler().fit_transform(X_train)\n",
    "    y_train = StandardScaler().fit_transform(y_train.reshape(-1, 1))\n",
    "    top_models, _, _ = TopRsq(X_train, y_train, m=n_informative)\n",
    "    true_indices = np.where(top_models[:, 0] > 0)[0]\n",
    "    est_indices, ratings = comprehensive_search(X_train, y_train, max_support_size=max_support_size, threshold=threshold)\n",
    "    if est_indices is not None and len(true_indices) == len(est_indices) and np.all(true_indices == est_indices):\n",
    "        success += 1\n",
    "        \n",
    "success/n_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c53fb0-2189-45e9-abec-70257e4ae903",
   "metadata": {},
   "source": [
    "### PDE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37423912-fac0-4199-bd39-04622baa3bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise level: 50.0\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./Datasets/\"\n",
    "data = sio.loadmat(os.path.join(data_path, \"KdV_rudy.mat\"))\n",
    "u_clean = (data['usol']).real; u = u_clean.copy()\n",
    "x = (data['x'][0]).real\n",
    "t = (data['t'][:,0]).real\n",
    "dt = t[1]-t[0]; dx = x[2]-x[1]\n",
    "\n",
    "np.random.seed(0)\n",
    "noise_type = \"gaussian\"\n",
    "noise_lv = float(50)\n",
    "print(\"Noise level:\", noise_lv)\n",
    "noise = 0.01*np.abs(noise_lv)*(u.std())*np.random.randn(u.shape[0],u.shape[1])\n",
    "u = u + noise\n",
    "u = np.load(\"./Denoised_data/kdv_gaussian50_bm3d.npy\")\n",
    "\n",
    "xt = np.array([x.reshape(-1, 1), t.reshape(1, -1)], dtype=object)\n",
    "X, T = np.meshgrid(x, t)\n",
    "XT = np.asarray([X, T]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b6b9e2-e6a1-4b76-ac70-d22a829fccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_library = ps.PolynomialLibrary(degree=2, include_bias=False)\n",
    "\n",
    "weak_lib = ps.WeakPDELibrary(\n",
    "    function_library=function_library,\n",
    "    derivative_order=3,\n",
    "    spatiotemporal_grid=XT,\n",
    "    include_bias=True,\n",
    "    diff_kwargs={\"is_uniform\":True},\n",
    "    K=10000\n",
    ")\n",
    "\n",
    "X_pre = np.array(weak_lib.fit_transform(np.expand_dims(u, -1)))\n",
    "y_pre = weak_lib.convert_u_dot_integral(np.expand_dims(u, -1))\n",
    "y_stan = StandardScaler().fit_transform(y_pre)\n",
    "N = len(y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e1887bf-fbc0-4d06-bc0b-1934d8bd56d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5, 6]),\n",
       " array([[0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [1.        , 1.        , 0.59792465, 0.50007196],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.57037676, 0.25715582, 1.        , 1.        ],\n",
       "        [0.96889968, 0.75781667, 0.80035118, 1.        ],\n",
       "        [0.88294231, 0.25069132, 0.59787182, 0.49998517],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.78206225, 0.49947882, 0.39822583, 0.49997189],\n",
       "        [0.76565043, 0.24996776, 0.20007144, 0.49997098]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effective_indices, rating = comprehensive_search(X_pre, y_stan, \n",
    "                                                 max_support_size=max_support_size,  \n",
    "                                                 threshold=0.75)\n",
    "effective_indices, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8660681-9719-4ca7-b66f-fbfab0f13a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3]), array([5, 6]), array([3, 5, 6]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(TopRsq(X_pre, y_pre, m=1)[0][:, 0])[0], \\\n",
    "np.nonzero(TopRsq(X_pre, y_pre, m=2)[0][:, 0])[0], \\\n",
    "np.nonzero(TopRsq(X_pre, y_pre, m=3)[0][:, 0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b0f1af4-b9ae-4468-a26c-6a8a12230589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exhaustive_selection import best_subset\n",
    "from functools import partial\n",
    "from p_tqdm import p_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "544f354d-3f3f-498f-bd24-fa6a4a105233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e73e517e8cf48878e14e92381063367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(3,), (5, 6), (3, 5, 6), (3, 5, 6, 7), (3, 5, 6, 7, 10)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_subset_results = p_map(partial(best_subset, X_pre, y_pre), [1, 2, 3, 4, 5])\n",
    "best_subset_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14e1078-0e03-4c15-b489-6e9255b27897",
   "metadata": {},
   "source": [
    "### Exat model evidence ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "377ebff4-a13f-4535-a313-a06ee420b29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5791.343919431005,\n",
       "  5845.8666839311,\n",
       "  5843.473219566753,\n",
       "  5843.443861699447,\n",
       "  5842.85127357445],\n",
       " 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bme = [log_evidence(X_pre, y_pre, effective_indices, v=1e-2, standardize=False) for effective_indices in best_subset_results]\n",
    "bme, np.argmax(bme)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43693306-75a9-4019-980e-4528eb917252",
   "metadata": {},
   "source": [
    "### Bayes factor ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c5a118e-986c-4fa4-9835-a515d04ac89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongpisitthanasutives/Desktop/research/pocomc/pocomc/mcmc.py:132: RuntimeWarning: overflow encountered in exp559]    \n",
      "  np.exp(logl_prime * beta - logl * beta + logp_prime - logp + logdetj_prime - logdetj + logdetj_flow_prime - logdetj_flow - A + B)\n",
      "Iter: 49it [00:51,  1.05s/it, beta=1, calls=111104, ESS=3841, logZ=3.19e+4, logP=3.19e+4, acc=0.352, steps=10, eff=0.588]\n"
     ]
    }
   ],
   "source": [
    "effective_indices = [5, 6]\n",
    "X_pre_sub = X_pre[:, effective_indices].copy().T\n",
    "def log_likelihood(param):\n",
    "    global X_pre_sub, y_pre, N\n",
    "    ssr = np.sum(np.abs(param@X_pre_sub - y_pre.flatten())**2, axis=-1)\n",
    "    def ssr2llf(ssr, nobs):\n",
    "        nobs2 = nobs / 2.0\n",
    "        llf = -nobs2 * np.log(2 * np.pi) - nobs2 * np.log(ssr / nobs) - nobs2\n",
    "        return llf\n",
    "    return ssr2llf(ssr, N)\n",
    "\n",
    "n_dim = len(effective_indices)\n",
    "prior = pc.Prior(n_dim*[norm(0, 1)])\n",
    "sampler = pc.Sampler(\n",
    "    prior=prior,\n",
    "    likelihood=log_likelihood,\n",
    "    vectorize=True,\n",
    ")\n",
    "sampler.run()\n",
    "simple_samples, simple_weights, _, _  = sampler.posterior()\n",
    "logz_simple, logz_err_simple = sampler.evidence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7236344b-ccee-4e6b-8e42-370321cff474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 59it [01:11,  1.21s/it, beta=1, calls=114944, ESS=3875, logZ=3.52e+4, logP=3.53e+4, acc=0.164, steps=10, eff=0.699]  \n"
     ]
    }
   ],
   "source": [
    "effective_indices = [3, 5, 6]\n",
    "X_pre_sub = X_pre[:, effective_indices].copy().T\n",
    "def log_likelihood(param):\n",
    "    global X_pre_sub, y_pre, N\n",
    "    ssr = np.sum(np.abs(param@X_pre_sub - y_pre.flatten())**2, axis=-1)\n",
    "    def ssr2llf(ssr, nobs):\n",
    "        nobs2 = nobs / 2.0\n",
    "        llf = -nobs2 * np.log(2 * np.pi) - nobs2 * np.log(ssr / nobs) - nobs2\n",
    "        return llf\n",
    "    return ssr2llf(ssr, N)\n",
    "\n",
    "n_dim = len(effective_indices)\n",
    "prior = pc.Prior(n_dim*[norm(0, 1)])\n",
    "sampler = pc.Sampler(\n",
    "    prior=prior,\n",
    "    likelihood=log_likelihood,\n",
    "    vectorize=True,\n",
    ")\n",
    "sampler.run()\n",
    "extended_samples, extended_weights, _, _  = sampler.posterior()\n",
    "logz_extended, logz_err_extended = sampler.evidence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be17165b-4158-4fc0-a1d9-73a82c051781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extended model is more probable than the simple model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/93/k1wfjtks3ln5whhs7nszdspw0000gn/T/ipykernel_1472/862312044.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  BF = np.exp(logz_extended-logz_simple)\n"
     ]
    }
   ],
   "source": [
    "# Bayes factor of extended to simple model\n",
    "BF = np.exp(logz_extended-logz_simple)\n",
    "if BF > 1:\n",
    "    print('The extended model is more probable than the simple model.')\n",
    "else:\n",
    "    print('The simple model is more probable than the extended model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5ade9e3-8447-4d8b-9097-b43892b6721f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6624994 , -5.11376209])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(simple_weights, simple_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6edc0f5d-3887-40c8-a637-5106ea332f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15187746, -0.80474109, -4.96284739])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(extended_weights, extended_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a668f5-c976-44ab-9800-a24a4fdbe692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da46bba4-8679-426b-b44e-c8d0e995064a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sindy]",
   "language": "python",
   "name": "conda-env-sindy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
