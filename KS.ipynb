{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28567c5-c0e2-4efa-98a0-63961e04e39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Author: Pongpisit Thanasutives ###\n",
    "import os\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from tqdm import trange\n",
    "from scipy import io as sio\n",
    "from scipy.stats import uniform, norm\n",
    "import pysindy as ps\n",
    "import pocomc as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ae7ab1-29da-46ac-b917-f1c6ead4b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### algorithm 2 ###\n",
    "def TopRsq(X_full, y, m, n_tops=25):\n",
    "    n_feats = X_full.shape[-1]\n",
    "    r_scores = []\n",
    "    models = []\n",
    "    for comb in combinations(range(n_feats), m):\n",
    "        comb = list(comb)\n",
    "        active_indices = np.zeros(n_feats)\n",
    "        active_indices[comb] = 1\n",
    "        X_sub = X_full[:, comb]\n",
    "        lr = LinearRegression(fit_intercept=False).fit(X_sub, y)\n",
    "        R2 = lr.score(X_sub, y)\n",
    "        r_scores.append(R2)\n",
    "        models.append(active_indices)\n",
    "    r_scores = np.array(r_scores)\n",
    "    r_argsort = np.argsort(r_scores)[::-1][:n_tops]\n",
    "    r_scores = r_scores[r_argsort]\n",
    "    models = np.array(models)\n",
    "    models = models[r_argsort].T\n",
    "    rating = np.dot(models, r_scores)\n",
    "    return models, r_scores, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67516471-5f91-4578-a979-61fc4a2c82f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### algorithm 3 ###\n",
    "def comprehensive_search(X_full, y, max_support_size=8, n_tops=None, threshold=0.75):\n",
    "    X = X_full.copy()\n",
    "    n_feats = X_full.shape[-1]\n",
    "    n_tops = int(np.ceil(n_feats/2)) if n_tops is None else n_tops\n",
    "    ratings = np.zeros((n_feats, max_support_size))\n",
    "    search = True; support_size = 1\n",
    "    optimal_indices = None\n",
    "    i0 = np.arange(n_feats)\n",
    "    while search and support_size <= max_support_size:\n",
    "        _, _, rating = TopRsq(X, y, m=support_size, n_tops=n_tops)\n",
    "        rating = rating/rating.max()\n",
    "        ratings[:, support_size-1][i0] = rating\n",
    "        if support_size >= 2:\n",
    "            i0 = np.nonzero(ratings[:, support_size-1] + ratings[:, support_size-2])[0]\n",
    "            X = X_full[:, i0]\n",
    "            i1 = np.where(ratings[:, support_size-1] > threshold)[0]\n",
    "            i2 = np.where(ratings[:, support_size-2] > threshold)[0]\n",
    "            if len(i1) == len(i2) and np.all(i1 == i2):\n",
    "                search = False\n",
    "                optimal_indices = i1\n",
    "        support_size += 1\n",
    "    return optimal_indices, ratings[:, :support_size-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d70184eb-b539-4ae7-9fd5-4eb818c6ddf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:14<00:00,  6.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_experiments = 100\n",
    "n_samples = 10000\n",
    "n_features = 8\n",
    "n_informative = 2\n",
    "\n",
    "threshold = 0.25\n",
    "max_support_size = 8\n",
    "\n",
    "success = 0\n",
    "for i in trange(n_experiments):\n",
    "    X_train, y_train = make_regression(n_samples=n_samples, n_features=n_features, n_informative=n_informative)\n",
    "    X_train = StandardScaler().fit_transform(X_train)\n",
    "    y_train = StandardScaler().fit_transform(y_train.reshape(-1, 1))\n",
    "    top_models, _, _ = TopRsq(X_train, y_train, m=n_informative)\n",
    "    true_indices = np.where(top_models[:, 0] > 0)[0]\n",
    "    est_indices, ratings = comprehensive_search(X_train, y_train, max_support_size=max_support_size, threshold=threshold)\n",
    "    if est_indices is not None and len(true_indices) == len(est_indices) and np.all(true_indices == est_indices):\n",
    "        success += 1\n",
    "        \n",
    "success/n_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c53fb0-2189-45e9-abec-70257e4ae903",
   "metadata": {},
   "source": [
    "### PDE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37423912-fac0-4199-bd39-04622baa3bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'x', 'uu', 'tt'])\n",
      "Noise level: 50.0\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./Datasets/\"\n",
    "data = sio.loadmat(os.path.join(data_path, \"kuramoto_sivishinky.mat\"))\n",
    "print(data.keys())\n",
    "u_clean = (data['uu']).real; u = u_clean.copy()\n",
    "x = (data['x'][:, 0]).real\n",
    "t = (data['tt'][0]).real\n",
    "dt = t[1]-t[0]; dx = x[2]-x[1]\n",
    "\n",
    "np.random.seed(0)\n",
    "noise_type = \"gaussian\"\n",
    "noise_lv = float(50)\n",
    "print(\"Noise level:\", noise_lv)\n",
    "noise = 0.01*np.abs(noise_lv)*(u.std())*np.random.randn(u.shape[0],u.shape[1])\n",
    "u = u + noise\n",
    "u = np.load(\"./Denoised_data/ks_gaussian50_bm3d.npy\")\n",
    "\n",
    "xt = np.array([x.reshape(-1, 1), t.reshape(1, -1)], dtype=object)\n",
    "X, T = np.meshgrid(x, t)\n",
    "XT = np.asarray([X, T]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4b6b9e2-e6a1-4b76-ac70-d22a829fccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_library = ps.PolynomialLibrary(degree=2, include_bias=False)\n",
    "\n",
    "weak_lib = ps.WeakPDELibrary(\n",
    "    function_library=function_library,\n",
    "    derivative_order=4,\n",
    "    spatiotemporal_grid=XT,\n",
    "    include_bias=True,\n",
    "    diff_kwargs={\"is_uniform\":True},\n",
    "    K=1000\n",
    ")\n",
    "\n",
    "X_pre = np.array(weak_lib.fit_transform(np.expand_dims(u, -1)))\n",
    "y_pre = weak_lib.convert_u_dot_integral(np.expand_dims(u, -1))\n",
    "y_stan = StandardScaler().fit_transform(y_pre)\n",
    "N = len(y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e1887bf-fbc0-4d06-bc0b-1934d8bd56d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7]),\n",
       " array([[0.        , 0.        ],\n",
       "        [0.        , 0.14676205],\n",
       "        [0.00322766, 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.15307978],\n",
       "        [0.        , 0.        ],\n",
       "        [0.        , 0.10141713],\n",
       "        [1.        , 1.        ],\n",
       "        [0.00443764, 0.        ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.01979575, 0.1635375 ],\n",
       "        [0.13425153, 0.14912227],\n",
       "        [0.0059761 , 0.08962063],\n",
       "        [0.00430323, 0.08952405],\n",
       "        [0.00331661, 0.10693659]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effective_indices, rating = comprehensive_search(X_pre, y_stan, \n",
    "                                                 max_support_size=max_support_size,  \n",
    "                                                 threshold=0.75)\n",
    "effective_indices, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8660681-9719-4ca7-b66f-fbfab0f13a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7]), array([ 7, 10]), array([4, 6, 7]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nonzero(TopRsq(X_pre, y_pre, m=1)[0][:, 0])[0], \\\n",
    "np.nonzero(TopRsq(X_pre, y_pre, m=2)[0][:, 0])[0], \\\n",
    "np.nonzero(TopRsq(X_pre, y_pre, m=3)[0][:, 0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb488fb2-ca26-4aa5-b723-bf9e704ca50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "# efs = EFS(LinearRegression(fit_intercept=False), min_features=3, max_features=3, scoring='r2')\n",
    "# efs.fit(X_pre, y_pre)\n",
    "# efs.best_score_, efs.best_idx_, efs.best_feature_names_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14e1078-0e03-4c15-b489-6e9255b27897",
   "metadata": {},
   "source": [
    "### Exat model evidence ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "377ebff4-a13f-4535-a313-a06ee420b29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1235.1752294064306 -1239.731391641337\n",
      "-993.4991272855287 -1030.7564770809772\n",
      "382.0046233007934 -435.37046735765597\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import loggamma\n",
    "\n",
    "def log_evidence(X_full, y, effective_indices, v=1):\n",
    "    y = StandardScaler().fit_transform(y.reshape(-1, 1))\n",
    "    k = 1 + 1/v\n",
    "    N = len(y)\n",
    "    p = len(effective_indices)\n",
    "    K = X_full[:, effective_indices]\n",
    "    \n",
    "    KTy = K.T@y\n",
    "    yTy = y.T@y\n",
    "    \n",
    "    mu = np.linalg.lstsq(K, y, rcond=None)[0]\n",
    "    Sigma = np.diag(np.ones(p)) * (1 - p/N)/(yTy + mu.T@KTy)[0][0]\n",
    "    \n",
    "    Smu = Sigma@mu\n",
    "    A = K.T@K + Sigma\n",
    "    A_inv = np.linalg.pinv(A)\n",
    "    b = KTy + Smu\n",
    "    xi = (yTy + mu.T@Smu - b.T@(A_inv@b))[0][0]\n",
    "    \n",
    "    return N*((np.linalg.slogdet(Sigma)[1] - np.linalg.slogdet(A)[1])/(2*N) - 0.5*np.log(2*np.pi) - \\\n",
    "              (0.5 + k/N)*np.log(xi/2 + 1/v) - (k*np.log(v))/N + (loggamma(N/2 + k) - loggamma(k))/N)\n",
    "\n",
    "for effective_indices in [[7,], [7, 10], [4, 6, 7]]:\n",
    "    print(log_evidence(X_pre, y_pre, effective_indices, v=1), \n",
    "          log_evidence(X_pre, y_pre, effective_indices, v=1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43693306-75a9-4019-980e-4528eb917252",
   "metadata": {},
   "source": [
    "### Bayes factor ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c5a118e-986c-4fa4-9835-a515d04ac89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 32it [00:36,  1.13s/it, beta=1, calls=34304, ESS=3927, logZ=-1.27e+3, logP=-1.27e+3, acc=0.635, steps=3, eff=0.588]   \n"
     ]
    }
   ],
   "source": [
    "effective_indices = [7, 10]\n",
    "X_pre_sub = X_pre[:, effective_indices].copy().T\n",
    "def log_likelihood(param):\n",
    "    global X_pre_sub, y_pre, N\n",
    "    ssr = np.sum(np.abs(param@X_pre_sub - y_pre.flatten())**2, axis=-1)\n",
    "    def ssr2llf(ssr, nobs):\n",
    "        nobs2 = nobs / 2.0\n",
    "        llf = -nobs2 * np.log(2 * np.pi) - nobs2 * np.log(ssr / nobs) - nobs2\n",
    "        return llf\n",
    "    return ssr2llf(ssr, N)\n",
    "\n",
    "n_dim = len(effective_indices)\n",
    "prior = pc.Prior(n_dim*[norm(0, 1)])\n",
    "sampler = pc.Sampler(\n",
    "    prior=prior,\n",
    "    likelihood=log_likelihood,\n",
    "    vectorize=True,\n",
    ")\n",
    "sampler.run()\n",
    "simple_samples, simple_weights, _, _  = sampler.posterior()\n",
    "logz_simple, logz_err_simple = sampler.evidence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7236344b-ccee-4e6b-8e42-370321cff474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 36it [00:50,  1.40s/it, beta=1, calls=34560, ESS=3958, logZ=-1.26e+3, logP=-1.25e+3, acc=0.577, steps=7, eff=0.72]   \n"
     ]
    }
   ],
   "source": [
    "effective_indices = [4, 7, 10]\n",
    "X_pre_sub = X_pre[:, effective_indices].copy().T\n",
    "def log_likelihood(param):\n",
    "    global X_pre_sub, y_pre, N\n",
    "    ssr = np.sum(np.abs(param@X_pre_sub - y_pre.flatten())**2, axis=-1)\n",
    "    def ssr2llf(ssr, nobs):\n",
    "        nobs2 = nobs / 2.0\n",
    "        llf = -nobs2 * np.log(2 * np.pi) - nobs2 * np.log(ssr / nobs) - nobs2\n",
    "        return llf\n",
    "    return ssr2llf(ssr, N)\n",
    "\n",
    "n_dim = len(effective_indices)\n",
    "prior = pc.Prior(n_dim*[norm(0, 1)])\n",
    "sampler = pc.Sampler(\n",
    "    prior=prior,\n",
    "    likelihood=log_likelihood,\n",
    "    vectorize=True,\n",
    ")\n",
    "sampler.run()\n",
    "extended_samples, extended_weights, _, _  = sampler.posterior()\n",
    "logz_extended, logz_err_extended = sampler.evidence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be17165b-4158-4fc0-a1d9-73a82c051781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extended model is more probable than the simple model.\n"
     ]
    }
   ],
   "source": [
    "# Bayes factor of extended to simple model\n",
    "BF = np.exp(logz_extended-logz_simple)\n",
    "if BF > 1:\n",
    "    print('The extended model is more probable than the simple model.')\n",
    "else:\n",
    "    print('The simple model is more probable than the extended model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5ade9e3-8447-4d8b-9097-b43892b6721f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.57864742, -0.04596535])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(simple_weights, simple_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6edc0f5d-3887-40c8-a637-5106ea332f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08739292, -0.60465573, -0.03282478])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(extended_weights, extended_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a668f5-c976-44ab-9800-a24a4fdbe692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb5080-3707-4be3-95fb-a09e2b63abea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sindy]",
   "language": "python",
   "name": "conda-env-sindy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
